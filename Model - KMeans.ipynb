{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Documents with KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import gensim.utils\n",
    "import mpld3\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from functions import *\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = load_data('hard.cor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARD1 3455\n",
      "HARD2 502\n",
      "HARD3 376\n"
     ]
    }
   ],
   "source": [
    "for key in d.keys():\n",
    "    print key, len(d[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a documents of all sentences, regardless of meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_documents = [sentence for value in d.values() for sentence in value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'> 4333\n"
     ]
    }
   ],
   "source": [
    "print type(train_documents[0]), len(train_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to extract the labels of the documents into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_labels(d):\n",
    "    ys = []\n",
    "    for i in d.keys():\n",
    "        yi = np.repeat(str(i), len(d[i]))\n",
    "        ys.append(yi)\n",
    "    return np.concatenate(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = set_labels(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lem_train_documents = lem(train_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lose popular support someone have kill defeat do'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_train_documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectors, vectorizer = Tfidf(lem_train_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4333x73935 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 106933 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since KMeans Clustering is an unsupervised learning and does not require the labels. We can use the lemmatized training documents directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Get the number of meanings for the word we are working with\n",
    "n_meanings = len(d)\n",
    "print n_meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=3, n_init=100,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit KMeans model to our Tfidf vectors from the lemmatized training documents\n",
    "km = KMeans(n_clusters = n_meanings, n_init=100)\n",
    "km.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the predicted labels from KMeans model.\n",
    "# Labels are assigned as 0, 1, 2,..., (n_meanings-1). \n",
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3062,  408,  863])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of labels in each group. \n",
    "np.bincount(km.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that cluster labels do not correspond to the order of meaning in the dictionary. We will use the sizes of clusters to determine the meanings. Largest cluster corresponds to the most popular meaning in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Kmeans Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well the model performs, we will match the predicted labels (km.labels_) to the actual labels from the dictionary. We can also calculate the scores of our predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(km.labels_, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As we can see, our KMeans model tend to overpredict them most common meaning, the result is good in more common meaning and poor on less common meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Most Common Words in Each Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite not working well until we can optimize the model further, it is useful to see which words are most common in each cluster. We will use Latent Semantic Analysis (LSA) to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=3, n_iter=100,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit LSA model\n",
    "lsa = TruncatedSVD(n_components = n_meanings, n_iter=100)\n",
    "lsa.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Meaning: HARD1--- \n",
      "\n",
      "say\n",
      "time\n",
      "believe\n",
      "work\n",
      "make\n",
      "thing\n",
      "person\n",
      "know\n",
      "say say\n",
      "year\n",
      "\n",
      "---Meaning: HARD2--- \n",
      "\n",
      "time\n",
      "believe\n",
      "work\n",
      "make\n",
      "know\n",
      "come\n",
      "think\n",
      "just\n",
      "person\n",
      "thing\n",
      "\n",
      "---Meaning: HARD3--- \n",
      "\n",
      "time\n",
      "know\n",
      "right\n",
      "play\n",
      "way\n",
      "new\n",
      "real\n",
      "say time\n",
      "say\n",
      "feeling\n",
      "\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(lsa.components_):\n",
    "    comp_terms = zip(terms, comp)\n",
    "    sorted_terms = sorted(comp_terms, key = lambda x:x[1], reverse = True)[:10]\n",
    "    print \"---Meaning: {}--- \".format(d.keys()[i])\n",
    "    print\n",
    "    for term in sorted_terms:\n",
    "        print term[0]\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting \"HARD\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lim = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectors_limit = vectors[: lim, : lim]\n",
    "y_limit = y[: lim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use cosine distance to represent the Tfidf vectors\n",
    "dist = 1 - cosine_similarity(vectors_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MDS()\n",
    "\n",
    "# convert two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set up colors per clusters using a dict\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3'}\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: 'predict: HARD1',\n",
    "                 1: 'predict: HARD2', \n",
    "                 2: 'predict: HARD3', \n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = km.labels_[: lim].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 600, 600, 600)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xs), len(ys), len(clusters), len(y_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=y_limit)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HARD1</td>\n",
       "      <td>0.645055</td>\n",
       "      <td>0.161545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>HARD1</td>\n",
       "      <td>0.769988</td>\n",
       "      <td>0.048801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>HARD1</td>\n",
       "      <td>-0.606205</td>\n",
       "      <td>-0.119010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>HARD1</td>\n",
       "      <td>-0.218119</td>\n",
       "      <td>0.754435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>HARD1</td>\n",
       "      <td>-0.148321</td>\n",
       "      <td>0.767342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  title         x         y\n",
       "0      0  HARD1  0.645055  0.161545\n",
       "1      1  HARD1  0.769988  0.048801\n",
       "2      0  HARD1 -0.606205 -0.119010\n",
       "3      0  HARD1 -0.218119  0.754435\n",
       "4      0  HARD1 -0.148321  0.767342"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
    "            label=cluster_names[name], color=cluster_colors[name], \n",
    "            mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelbottom='off')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelleft='off')\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=8)  \n",
    "\n",
    "    \n",
    "    \n",
    "plt.show() #show the plot\n",
    "\n",
    "#uncomment the below to save the plot if need be\n",
    "plt.savefig('HARD_KMeams_clusters_small.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
